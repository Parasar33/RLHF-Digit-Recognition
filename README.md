# 🖊️ RLHF Digit Recognizer

An interactive digit recognition system built using PyTorch and Pygame, with a Reinforcement Learning from Human Feedback (RLHF) loop to improve predictions over time.  

You can:
- Draw digits in a 28×28 grid.
- Get real-time predictions from a trained neural network.
- Provide corrections, which are stored for future retraining.

📹 Project Demo:  
[![Watch the Demo](https://img.shields.io/badge/Video-Watch%20Now-blue)](https://github.com/Parasar33/RLHF-Digit-Recognition/blob/main/PR%20PBL%20Review%20Video.mp4?raw=true)  

Project Structure:
.
├── digit_recognizer.py     # Interactive app (inference + RLHF feedback)  
├── train_model.py          # Model training script (MNIST dataset)  
├── best_model.pth          # Trained model weights (generated by train_model.py)  
├── correction_data.json    # Stores RLHF user corrections  
├── README.md               # This file  

Installation:
```
git clone https://github.com/your-username/RLHF-Digit-Recognition.git
cd RLHF-Digit-Recognition/1_With RLHF
pip install torch torchvision pygame numpy matplotlib scikit-learn scipy
```

Usage:
1) Train the Model:  
```
python train_model.py
```
- Loads the MNIST dataset from `.idx` files  
- Trains the model with batch normalization and dropout  
- Saves the best model weights to `best_model.pth`  

2) Run the Interactive App:  
```
python digit_recognizer.py
```

Controls:
- Left Click & Drag → Draw on the 28×28 grid  
- C → Clear the grid  
- R → Record correction (prompts you for the correct digit)  
- ESC → Exit the app  

RLHF Workflow:
1) User draws a digit → Model predicts  
2) If wrong, user presses `R` and enters the correct digit  
3) Data is saved in `correction_data.json`  
4) Retrain the model with this correction data to improve accuracy  

Model Architecture:
- Fully connected feed-forward network  
- Layers: 784 → 512 → 256 → 128 → 10  
- Batch normalization, ReLU activation, dropout  
- Softmax output layer  
